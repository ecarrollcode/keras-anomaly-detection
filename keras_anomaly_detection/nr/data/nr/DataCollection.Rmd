---
title: "Collecting Training Data"
output:
  html_notebook: default
  pdf_document: default
---

We use a special New Relic R library available on github in the repo `bkayser/NewRelicR'.  
However there's an internal fork I created that is more efficient to use when you don't 
want to authenticate for each account accessed.  This fork is only available internally.

```{r, results='hide'}
# devtools::install('/Users/bill/workspace/NewRelicR', force=T)
library(newrelic)
```

Other libraries used:

```{r, results='hide'}
library(dplyr)
library(tibble)
library(knitr)
library(ggplot2)
library(reshape2)
library(rprojroot)
library(lubridate)
library(stringi)
library(purrr)
library(caret)
source('../../lib/nrdb-utils.R')
source('../../lib/cds-utils.R')
source('../../lib/cache.R')
source('../../lib/anonymize.R')
```

Other initialization:

```{r, results='hide'}
theme_set(theme_light())
knitr::opts_chunk$set(echo = T, messages=F, warning=F, error = F, cache=T)
```


<style type="text/css">
.table {
    width: inherit;
    max-width: 100%;
    margin-bottom: 20px;
}
.math.display {
  font-size: 28px;
}
</style>

## Data Collection Utility Functions

Some of the lower level utility functions are already defined in `cds-utils.R` and `nrdb-utils.R`.

Here is where we define the actual tables we're looking at and the columns we're extracting.

We use different servers for staging vs. production accounts.

```{r}
lookup_dirac_host <- function(account_id, infra=T) {
    if (account_id == 550352) # Staging accounts
        Sys.getenv('DIRAC_STAGING_HOST')
    else if (account_id == 1 && infra)
        Sys.getenv('DIRAC_STAGING_HOST')    
    else
        Sys.getenv('DIRAC_PROD_HOST')
}
```

-------------

Create a function to get Infrastructure data for an app.

```{r}

collect_infra_data <- function(account_id, app_id, begin_time, end_time, resolution=10) {
    cache_entry <- cache('infra', account_id, app_id, begin_time, end_time)
    if (cache_entry$exists()) return(cache_entry$get())
    queries <- c(
        paste0("SELECT average(swapFreeBytes)/1000000 as swapFreeMB, ",
               "min(memoryFreeBytes)/1000000 as minFreeMemMB, ",
               "min(diskFreePercent) as minDiskFreePct, ",
               "average(cpuUserPercent) as cpuUserPct, ",
               "average(cpuSystemPercent) as cpuSystemPct, ",
               "average(cpuStealPercent) as cpuStealPct, ",
               "average(cpuIOWaitPercent) as cpuIOWaitPct, ",
               "average(loadAverageOneMinute) as loadAverage, ",
               "average(diskReadUtilizationPercent) as diskReadPct, ",
               "average(diskWriteUtilizationPercent) as diskWritePct ",
               "from SystemSample where `nr.apmApplicationIds` like '%|", app_id, "|%'"),
        paste0("SELECT average(receiveBytesPerSecond) as receiveBytes, ",
               "average(transmitBytesPerSecond) as transmitBytes, ", 
               "average(transmitErrorsPerSecond) as transmitErrors ",
               "FROM NetworkSample ",
               "WHERE `nr.apmApplicationIds` like '%|", app_id, "|%'"),
        paste0("SELECT count(*) as event_count FROM InfrastructureEvent facet category ",
               "where category in ('sessions', 'config', 'alerts', 'services', 'kernel', 'system') and `nr.apmApplicationIds` like '%|", app_id, "|%'"))
    
    data <- lapply(queries, function(query) {
        nrdb_timeseries_batch(account_id = account_id,
                              nrql_query = query,
                              end_time=end_time,
                              begin_time=begin_time,
                              timeout=20000,
                              host=lookup_dirac_host(account_id),
                              resolution=resolution)
    })
    infra_data <- data[[1]]
    for (events in data[-1]) {
        if (!rlang::is_empty(events)) {
            # Replace missing with 0 count
            events[is.na(events)] <- 0
            infra_data <- left_join(infra_data, events, by = c("begin_time", "end_time"))
        }
    }
    cache_entry$put(infra_data)
}
```

------------
Create a function to get deployment event counts for an app.

```{r}

collect_deployment_events <- function(account_id, app_id, begin_time, end_time, resolution=10) {
    cache_entry <- cache('deploys', account_id, app_id, begin_time, end_time)
    if (cache_entry$exists()) return(cache_entry$get())

    query <- paste0("SELECT count(*) as deploys from AlertsRecentEvents ",
                    "WHERE targetInstanceId=", app_id, " AND ",
                    "targetType = 'Application' AND ",
                    "recentEventType='DEPLOYMENT' AND ",
                    "customerAccountId=", account_id)
    
    message("      ", stri_sub(query, length=25), "...")
    nrdb_timeseries_batch(account_id = 1,
                          nrql_query = query,
                          resolution=resolution,
                          end_time=end_time,
                          begin_time=begin_time,
                          timeout=40000,
                          host=lookup_dirac_host(account_id)) %>%
        cache_entry$put()
}
```



```{r}

collect_web_data <- function(account_id, app_id, begin_time, end_time, resolution=10) {
    cache_entry <- cache('web', account_id, app_id, begin_time, end_time)
    if (cache_entry$exists()) return(cache_entry$get())

    # Get the 85 percentile value to use as apdex t
    # Doesn't work on full time range--times out.
    query <- paste0('SELECT percentile(duration, 85) from Transaction where appId=', app_id,' since 1 days ago')
    message("      ", query)
    rs <- nrdb_query(account_id, nrql_query=query, timeout=40000, host=lookup_dirac_host(account_id))
    results <- list()
    apdex <- signif(unname(rs[1]), 2)

    attributes <- c(cpm='count(*)',
                    apdex=paste0('apdex(duration, ',apdex,')'),
                    cpm.database=paste0('sum(databaseCallCount)/', resolution),
                    duration.database='average(databaseDuration)',
                    duration='average(duration)',
                    cpm.external=paste0('sum(externalCallCount)/', resolution),
                    duration.external='average(externalDuration)',
                    length='average(`response.headers.contentLength`)',
                    sum.queueDuration='sum(queueDuration)',
                    error_pct='percentage(count(*), WHERE error IS TRUE)',
                    background_pct='percentage(count(*), WHERE transactionType = \'Other\')')

    queries <- list(

        paste0('SELECT ', paste0(attributes, ' as `', names(attributes), '`', collapse=', '), ' FROM Transaction extrapolate where appId=', app_id),
        paste0('SELECT count(*)/', resolution,' as cpm FROM Transaction where appId=', app_id, ' extrapolate facet httpResponseCode'),
        paste0('SELECT count(*)/', resolution,' as cpm FROM Transaction where appId=', app_id, ' extrapolate facet transactionType '),
        paste0('SELECT count(*)/', resolution,' as cpm, average(duration) as duration FROM Transaction where appId=', app_id, ' extrapolate facet name limit 20')
      )
        
    results <- lapply(queries, function(query) {
        query_cache <- cache('webquery', account_id, app_id, begin_time, end_time, openssl::md5(query))
        message("      ", stri_sub(query, length=35), "...: ", query_cache$filename)
        if (query_cache$exists()) {
            query_cache$get()
        } else {
            data <- nrdb_timeseries_batch(account_id = account_id,
                                                  nrql_query = query,
                                                  resolution=resolution,
                                                  end_time=end_time,
                                                  begin_time=begin_time,
                                                  timeout=40000,
                                                  host=lookup_dirac_host(account_id))
            query_cache$put(data)
        }
    })
    web_data <- results[[1]]
    for (rs in results[-1]) {
        if (!is.null(rs) && !is.null(rs[['begin_time']])) {
            rs[is.na(rs)] <- 0
            web_data <- left_join(web_data, rs, by = c("begin_time", "end_time"))
        }
    }
    # Rename some of the variables
    names(web_data) <- names(web_data) %>%
#        stri_replace_all_regex("\\.[yx]", "") %>%
        stri_replace_all_fixed("cpm_", "cpm.") %>%
        stri_replace_all_fixed("duration_", "duration.")
    
    # Make queue duration per request
    mutate(web_data, 
           queue_duration=sum.queueDuration/cpm) %>% 
        select(-sum.queueDuration, -apdex_count) %>%
        cache_entry$put()
}
```

------------
Put it all together.

```{r, echo=T}

assemble_data <- function(account_id, app_id, begin_time, end_time, deployments_app_id, resolution) {
    message("  Account ", account_id, ", app ", app_id,"...")
    message("  infrastructure...")
    data.infra <- collect_infra_data(account_id, app_id, begin_time, end_time, resolution)
    message("  application...")
    data.app <- collect_web_data(account_id, app_id, begin_time, end_time, resolution)
    if (is.null(data.app) || is.null(data.infra)) {
        message("  ...not enough data available, skipping.")
        return(NULL)
    }
    data.deploys <- collect_deployment_events(account_id, deployments_app_id, begin_time, end_time, resolution)

    z <- full_join(data.infra, data.app, by=c('begin_time', 'end_time')) %>%
        full_join(data.deploys, by=c('begin_time', 'end_time')) %>%
        mutate(account_id=account_id, app_id=app_id) %>% 
        select(account_id, app_id, begin_time, end_time, everything()) %>%
        arrange(begin_time)
}
```

## Collect Data from APM and Browser Services

Target Applications:

```{r, results='asis', echo=F}
#end_time <- floor_date(Sys.time(), unit='days')
end_time <-  as.POSIXct(ymd('2017-10-30'))
begin_time <- end_time - ddays(28)
resolution <- 1

applications <- bind_rows(
    list(account_id = 1, app_id = 238575, name = "RPM-UI", deployments_app_id=1441),
    list(account_id = 1, app_id = 127809, name = 'Alerts-UI'),
    list(account_id = 550352, app_id = 701685, name = 'Dirac-Browser'),
    list(account_id = 550352, app_id = 993445, name = 'Browser-Monitoring-Service-Production'),
    list(account_id = 550352, app_id = 747163, name = 'BST-Cassandra'))

cat(paste0("* [", applications$name, "]",
           "(https://staging.newrelic.com/accounts/",applications$account_id, "/applications/", applications$app_id, ")\n"))
```

Collect data from `r begin_time` to `r end_time`.

```{r, eval=T}
timestamp <- strftime(begin_time, '%Y-%m-%d')
datasets <- list()
days <- as.numeric(end_time - begin_time, unit='days')
for (i in seq_along(applications$app_id)) {
    account_id <- applications$account_id[i]
    app_id <- applications$app_id[i]
    app_name <- applications$name[i]
    deployments_app_id <- 
        if (is.null(applications[['deployments_app_id']][i])) {
            app_id 
        } else {
            applications[['deployments_app_id']][i]
        }
    filename <- rprojroot::find_rstudio_root_file(paste0("data/nr/", days, "days/", resolution, "m/ts-", account_id, "-", app_name, "-", timestamp))
    message("Preparing for: ", filename, " ...")
    dir.create(dirname(dirname(filename)), showWarnings = F)
    dir.create(dirname(filename), showWarnings = F)
    if (!file.exists(paste0(filename, ".csv.gz"))) {
        data <- assemble_data(account_id, app_id, begin_time, end_time, deployments_app_id, resolution) %>%
            select(-account_id, -app_id) %>%
            mutate(app_name = app_name) %>%
            select(app_name, begin_time, end_time, cpm, duration, everything()) %>%
            arrange(begin_time)
        write.csv(data, gzfile(paste0(filename,'.csv.gz')))
        saveRDS(data, paste0(filename, '.RData'))
        datasets[[length(datasets)+1]] <- data
    }
}
```

## Collect Data from External Accounts

Target Applications:

```{r, results='asis', echo=F}
end_time.cust <-  as.POSIXct(ymd('2017-11-06'))
begin_time.cust <- end_time.cust - ddays(7)
resolution <- 1
applications <- bind_rows(
    list(account_id = 59, app_id = 3324764, account_name = "A", name = "SocialApp"),
    list(account_id = 59, app_id = 28312675, account_name = "A", name = "BlogApp"),
    list(account_id = 59, app_id = 27454151, account_name = "A", name = "VideoApp"),
    list(account_id = 59, app_id = 12612204, account_name = "A", name = "CMSApp")
)
cat(paste0("* Account ", applications$account_name, ": [", applications$name, "]",
           "(https://staging.newrelic.com/accounts/",applications$account_id, "/applications/", applications$app_id, ")\n"))
```

Collect data from `r begin_time.cust` to `r end_time.cust`, anonymizing transaction names.

```{r, eval=T}
timestamp <- strftime(begin_time.cust, '%Y-%m-%d')
datasets <- list()
days <- as.numeric(end_time.cust - begin_time.cust, unit='days')
for (i in seq_along(applications$app_id)) {
    account_id <- applications$account_id[i]
    app_id <- applications$app_id[i]
    app_name <- applications$name[i]
    account_name <- applications$account_name[1]
    filename <- rprojroot::find_rstudio_root_file(paste0("data/cust/", days, "days/", resolution, "m/ts-", account_name, "-", app_name, "-", timestamp))
    message("Preparing for: ", filename, " ...")
    dir.create(dirname(dirname(filename)), showWarnings = F)
    dir.create(dirname(filename), showWarnings = F)
    if (!file.exists(paste0(filename, ".csv.gz"))) {
        data <- assemble_data(account_id, app_id, begin_time.cust, end_time.cust, app_id, resolution) %>%
            select(-account_id, -app_id) %>%
            mutate(app_name = app_name) %>%
            select(app_name, begin_time, end_time, cpm, duration, everything()) %>%
            arrange(begin_time)
        names(data) <- anonymize_columns(names(data))
        write.csv(data, gzfile(paste0(filename,'.csv.gz')))
        saveRDS(data, paste0(filename, '.RData'))
        datasets[[length(datasets)+1]] <- data
    }
}
```


## Exploring the Data

### How much data in the New Relic datasets?

```{r}
files <- list.files("./28days/1m", "*.RData", full.names=T) 
data.list <- lapply(files, readRDS)
data.all <- bind_rows(data.list)
ggplot(data.all) +
    ggtitle("CPU", subtitle="Infrastructure Metric") +
    aes(x=begin_time, y=cpuUserPct) +
    geom_line() +
    facet_grid(app_name ~ ., scales='free_y')
ggplot(data.all) +
    ggtitle("Throughput", subtitle="Transaction Metric") +
    aes(x=begin_time, y=cpm) +
    geom_line() +
    facet_grid(app_name ~ ., scales='free_y')
ggplot(data.all) +
    ggtitle("Response Time", subtitle="Transaction Metric") +    
    aes(x = begin_time, y = duration) +
    geom_line() +
    ggtitle('Response Time') +
    facet_grid(app_name ~ ., scales='free_y')
```


### How much data in the External datasets?

```{r}
files <- list.files("../cust/7days/1m", "*.RData", full.names=T) 
data.list <- lapply(files, readRDS)
data.all <- bind_rows(data.list) %>% select(app_name, begin_time, cpuUserPct, cpm, duration)
ggplot(data.all) +
    ggtitle("CPU", subtitle="Infrastructure Metric") +
    aes(x=begin_time, y=cpuUserPct) +
    geom_line() +
    facet_grid(app_name ~ ., scales='free_y')
ggplot(data.all) +
    ggtitle("Throughput", subtitle="Transaction Metric") +
    aes(x=begin_time, y=cpm) +
    geom_line() +
    facet_grid(app_name ~ ., scales='free_y')
ggplot(data.all) +
    ggtitle("Response Time", subtitle="Transaction Metric") +    
    aes(x = begin_time, y = duration) +
    geom_line() +
    ggtitle('Response Time') +
    facet_grid(app_name ~ ., scales='free_y')
```


